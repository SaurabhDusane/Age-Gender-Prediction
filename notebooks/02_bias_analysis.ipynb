{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Analysis Across Demographic Groups\n",
    "\n",
    "This notebook analyzes model performance across different demographic groups to identify and measure bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_by_gender(results_df):\n",
    "    \"\"\"Analyze model performance by gender\"\"\"\n",
    "    \n",
    "    gender_groups = results_df.groupby('true_gender')\n",
    "    \n",
    "    metrics = []\n",
    "    for gender, group in gender_groups:\n",
    "        mae = np.mean(np.abs(group['predicted_age'] - group['true_age']))\n",
    "        accuracy = accuracy_score(group['true_gender'], group['predicted_gender']) * 100\n",
    "        \n",
    "        metrics.append({\n",
    "            'gender': gender,\n",
    "            'age_mae': mae,\n",
    "            'gender_accuracy': accuracy,\n",
    "            'count': len(group)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(\"Performance by Gender:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].bar(metrics_df['gender'], metrics_df['age_mae'])\n",
    "    axes[0].set_title('Age MAE by Gender')\n",
    "    axes[0].set_ylabel('MAE (years)')\n",
    "    \n",
    "    axes[1].bar(metrics_df['gender'], metrics_df['gender_accuracy'])\n",
    "    axes[1].set_title('Gender Classification Accuracy')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].axhline(y=95, color='r', linestyle='--', label='Target (95%)')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance by Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_by_ethnicity(results_df):\n",
    "    \"\"\"Analyze model performance by ethnicity\"\"\"\n",
    "    \n",
    "    ethnicity_groups = results_df.groupby('ethnicity')\n",
    "    \n",
    "    metrics = []\n",
    "    for ethnicity, group in ethnicity_groups:\n",
    "        mae = np.mean(np.abs(group['predicted_age'] - group['true_age']))\n",
    "        \n",
    "        metrics.append({\n",
    "            'ethnicity': ethnicity,\n",
    "            'age_mae': mae,\n",
    "            'count': len(group)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(\"Performance by Ethnicity:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(metrics_df['ethnicity'], metrics_df['age_mae'])\n",
    "    plt.title('Age MAE by Ethnicity')\n",
    "    plt.ylabel('MAE (years)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=4.0, color='r', linestyle='--', label='Target MAE (4.0)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics(results_df):\n",
    "    \"\"\"Calculate fairness metrics\"\"\"\n",
    "    \n",
    "    # Demographic Parity\n",
    "    gender_positive_rates = results_df.groupby('true_gender')['predicted_gender'].apply(\n",
    "        lambda x: (x == 'male').mean()\n",
    "    )\n",
    "    demographic_parity = gender_positive_rates.max() - gender_positive_rates.min()\n",
    "    \n",
    "    print(f\"Demographic Parity Difference: {demographic_parity:.4f}\")\n",
    "    print(f\"  (Lower is better, < 0.1 is considered fair)\\n\")\n",
    "    \n",
    "    # Performance Disparity\n",
    "    gender_mae = results_df.groupby('true_gender').apply(\n",
    "        lambda x: np.mean(np.abs(x['predicted_age'] - x['true_age']))\n",
    "    )\n",
    "    performance_disparity = gender_mae.max() - gender_mae.min()\n",
    "    \n",
    "    print(f\"Performance Disparity (Age MAE): {performance_disparity:.2f} years\")\n",
    "    print(f\"  (Lower is better)\\n\")\n",
    "    \n",
    "    return {\n",
    "        'demographic_parity': demographic_parity,\n",
    "        'performance_disparity': performance_disparity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intersectional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectional_analysis(results_df):\n",
    "    \"\"\"Analyze performance across intersectional groups\"\"\"\n",
    "    \n",
    "    intersectional = results_df.groupby(['true_gender', 'ethnicity']).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'age_mae': np.mean(np.abs(x['predicted_age'] - x['true_age'])),\n",
    "            'count': len(x)\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    pivot = intersectional.pivot(index='ethnicity', columns='true_gender', values='age_mae')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd')\n",
    "    plt.title('Age MAE by Gender and Ethnicity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return intersectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Analysis\n",
    "\n",
    "**Note:** Load your evaluation results with demographic labels before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# results_df = pd.read_csv('evaluation_results.csv')\n",
    "# gender_metrics = analyze_performance_by_gender(results_df)\n",
    "# ethnicity_metrics = analyze_performance_by_ethnicity(results_df)\n",
    "# fairness_metrics = calculate_fairness_metrics(results_df)\n",
    "# intersectional_metrics = intersectional_analysis(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
