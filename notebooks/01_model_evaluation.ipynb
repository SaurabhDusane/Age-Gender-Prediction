{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Benchmark Datasets\n",
    "\n",
    "This notebook evaluates the demographic analysis models on standard benchmark datasets:\n",
    "- UTKFace\n",
    "- FairFace\n",
    "- AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.core.pipeline import DemographicPipeline\n",
    "from src.models.age_estimator import AgeEstimator\n",
    "from src.models.gender_classifier import GenderClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Age Estimation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_age_estimation(model, test_data):\n",
    "    \"\"\"Evaluate age estimation performance\"\"\"\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for img, true_age in tqdm(test_data):\n",
    "        result = model.predict(img)\n",
    "        predictions.append(result['age'])\n",
    "        ground_truth.append(true_age)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - ground_truth))\n",
    "    rmse = np.sqrt(np.mean((predictions - ground_truth) ** 2))\n",
    "    cs_5 = np.mean(np.abs(predictions - ground_truth) <= 5) * 100\n",
    "    cs_10 = np.mean(np.abs(predictions - ground_truth) <= 10) * 100\n",
    "    \n",
    "    print(f\"Age Estimation Metrics:\")\n",
    "    print(f\"  MAE: {mae:.2f} years\")\n",
    "    print(f\"  RMSE: {rmse:.2f} years\")\n",
    "    print(f\"  CS@5: {cs_5:.2f}%\")\n",
    "    print(f\"  CS@10: {cs_10:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'cs_5': cs_5,\n",
    "        'cs_10': cs_10,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gender Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_gender_classification(model, test_data):\n",
    "    \"\"\"Evaluate gender classification performance\"\"\"\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for img, true_gender in tqdm(test_data):\n",
    "        result = model.predict(img)\n",
    "        predictions.append(result['gender'])\n",
    "        ground_truth.append(true_gender)\n",
    "    \n",
    "    accuracy = accuracy_score(ground_truth, predictions) * 100\n",
    "    precision = precision_score(ground_truth, predictions, average='binary', pos_label='male') * 100\n",
    "    recall = recall_score(ground_truth, predictions, average='binary', pos_label='male') * 100\n",
    "    f1 = f1_score(ground_truth, predictions, average='binary', pos_label='male') * 100\n",
    "    \n",
    "    print(f\"Gender Classification Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"  Precision: {precision:.2f}%\")\n",
    "    print(f\"  Recall: {recall:.2f}%\")\n",
    "    print(f\"  F1-Score: {f1:.2f}%\")\n",
    "    \n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Gender Classification Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_age_predictions(predictions, ground_truth):\n",
    "    \"\"\"Plot age predictions vs ground truth\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ground_truth, predictions, alpha=0.5)\n",
    "    plt.plot([0, 100], [0, 100], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('Ground Truth Age')\n",
    "    plt.ylabel('Predicted Age')\n",
    "    plt.title('Age Prediction: Ground Truth vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    errors = predictions - ground_truth\n",
    "    plt.hist(errors, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Prediction Error (years)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Age Prediction Error Distribution')\n",
    "    plt.axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation\n",
    "\n",
    "**Note:** You need to download and prepare the benchmark datasets before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and modify with your dataset path)\n",
    "# age_estimator = AgeEstimator(device='cuda', ensemble=True)\n",
    "# gender_classifier = GenderClassifier(device='cuda')\n",
    "\n",
    "# Load your test data\n",
    "# test_data_age = load_utkface_dataset('path/to/utkface')\n",
    "# test_data_gender = load_fairface_dataset('path/to/fairface')\n",
    "\n",
    "# age_results = evaluate_age_estimation(age_estimator, test_data_age)\n",
    "# gender_results = evaluate_gender_classification(gender_classifier, test_data_gender)\n",
    "\n",
    "# plot_age_predictions(age_results['predictions'], age_results['ground_truth'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
